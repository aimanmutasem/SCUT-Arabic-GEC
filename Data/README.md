# Dataset 
The datasets we used are [Qatar Arabic Language Bank](http://nlp.qatar.cmu.edu/qalb/) (QALB) from the second QALB shared task and Alwatan Arabic articles to generate the synthetic parallel corpus. The data of QALB corpus comes from the online commenters written at Aljazeera Arabic news channel articles. The release of QALB at ANLPACL 2015, includes non-native data comes from Arabic Learners Written Corpus (CERCLL) and some machine translation data is obtained from Wikipedia articles translated in Arabic language using Google translation. The training dataset contains 2 million words annotated and corrected by native Arabic speakers. 

Alwatan Arabic news articles corpus contains 20,291 articles and 10,000,000 words are categorized into six groups collected from the Omani newspaper. In our work, we generated a [training dataset](https://shorturl.at/chyCQ) containing 18,061,610 million words in training and validation sets. The fine-tuning set consists of training and validation sets, and we used AQLB test set for testing. The whole training dataset contains 20,285,278 words and divided into a synthetic and authentic set. 
